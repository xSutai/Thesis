{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZsN6JRbXx9X",
      "metadata": {
        "id": "xZsN6JRbXx9X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from patchify import patchify  #Only to handle large images\n",
        "import random\n",
        "from scipy import ndimage\n",
        "from datasets import Dataset as ds\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import SamModel\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import monai\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import threshold, normalize\n",
        "import cv2 as cv\n",
        "import csv\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748c77e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "executionInfo": {
          "elapsed": 483636,
          "status": "error",
          "timestamp": 1747504403093,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "748c77e1",
        "outputId": "74652f5f-7d78-45c8-f5d7-39b9fff878d9"
      },
      "outputs": [],
      "source": [
        "# The following code is based on the following github project:\n",
        "# Bhattiprolu, S. (2023). Fine-tune Segment Anything Model (SAM) - \n",
        "# Mitochondria Segmentation [Jupyter notebook]. \n",
        "# GitHub. https://github.com/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb\n",
        "\n",
        "# Portions of this code were generated with the assistance of ChatGPT (OpenAI, 2025) and subsequently modified by the author.\n",
        "# OpenAI. (2025). ChatGPT (May 2025 version) [Large language model]. https://chat.openai.com\n",
        "\n",
        "trainDirImage = \"/content/BrG_training_data/all_im/\"\n",
        "#trainDirImage2 = \"D:/Thesis/datasets/DRIVE/training/images/\"\n",
        "trainDirMask = \"/content/BrG_training_data/all_mask_cup/\"\n",
        "#trainDirMask2 = \"D:/Thesis/datasets/DRIVE/training/1st_manual/\"\n",
        "\n",
        "files1 = sorted([f for f in os.listdir(trainDirImage) if os.path.isfile(os.path.join(trainDirImage, f))])\n",
        "files2 = sorted([f for f in os.listdir(trainDirMask) if os.path.isfile(os.path.join(trainDirMask, f))])\n",
        "\n",
        "large_images = []\n",
        "large_masks = []\n",
        "\n",
        "h = 511\n",
        "w = 503\n",
        "\n",
        "for i in range(len( os.listdir(trainDirImage))):\n",
        "    if files1[i].endswith('.png'):\n",
        "        img = cv.imread(cv.samples.findFile(trainDirImage + files1[i]))\n",
        "        img = cv.cvtColor(img,cv.COLOR_BGR2RGB)\n",
        "        R,G,B = cv.split(img)\n",
        "        img = np.asarray(G)\n",
        "        large_images += [img]\n",
        "\n",
        "\n",
        "for i in  range(len(os.listdir(trainDirMask))):\n",
        "    if files2[i].endswith('.png'):\n",
        "        img = cv.imread(cv.samples.findFile(trainDirMask + files2[i]))\n",
        "        img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "        img = np.asarray(img)\n",
        "        large_masks += [img]\n",
        "\n",
        "#Desired patch size for smaller images and step size.\n",
        "patch_size = 250\n",
        "step = 250\n",
        "\n",
        "all_img_patches = []\n",
        "for img in range(len(large_images)):\n",
        "    large_image = np.asarray(large_images[img])\n",
        "    patches_img = patchify(large_image, (patch_size, patch_size), step=step)\n",
        "    for i in range(patches_img.shape[0]):\n",
        "        for j in range(patches_img.shape[1]):\n",
        "\n",
        "            single_patch_img = patches_img[i,j,:,:]\n",
        "            all_img_patches.append(single_patch_img)\n",
        "\n",
        "images = np.array(all_img_patches)\n",
        "\n",
        "#Let us do the same for masks\n",
        "all_mask_patches = []\n",
        "for img in range(len(large_masks)):\n",
        "    large_mask = np.asarray(large_masks[img])\n",
        "    patches_mask = patchify(large_mask, (patch_size, patch_size), step=step)  #Step=256 for 256 patches means no overlap\n",
        "\n",
        "    for i in range(patches_mask.shape[0]):\n",
        "        for j in range(patches_mask.shape[1]):\n",
        "\n",
        "            single_patch_mask = patches_mask[i,j,:,:]\n",
        "            single_patch_mask = (single_patch_mask / 255.).astype(np.uint8)\n",
        "            all_mask_patches.append(single_patch_mask)\n",
        "\n",
        "masks = np.array(all_mask_patches)\n",
        "\n",
        "# Create a list to store the indices of non-empty masks\n",
        "valid_indices = [i for i, mask in enumerate(masks) if mask.max() != 0]\n",
        "# Filter the image and mask arrays to keep only the non-empty pairs\n",
        "filtered_images = images[valid_indices]\n",
        "filtered_masks = masks[valid_indices]\n",
        "print(\"Image shape:\", filtered_images.shape)  # e.g., (num_frames, height, width, num_channels)\n",
        "print(\"Mask shape:\", filtered_masks.shape)\n",
        "\n",
        "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
        "dataset_dict = {\n",
        "    \"image\": [Image.fromarray(img) for img in filtered_images],\n",
        "    \"label\": [Image.fromarray(mask) for mask in filtered_masks],\n",
        "}\n",
        "\n",
        "# Create the dataset using the datasets.Dataset class\n",
        "dataset = ds.from_dict(dataset_dict)\n",
        "\n",
        "img_num = random.randint(0, filtered_images.shape[0]-1)\n",
        "example_image = dataset[img_num][\"image\"]\n",
        "example_mask = dataset[img_num][\"label\"]\n",
        "\n",
        "#Get bounding boxes from mask.\n",
        "def get_bounding_box(ground_truth_map):\n",
        "  # Define the size of your array\n",
        "  array_size = 250\n",
        "\n",
        "  # Define the size of your grid\n",
        "  grid_size = 15\n",
        "\n",
        "  # Generate the grid points\n",
        "  x = np.linspace(0, array_size-1, grid_size)\n",
        "  y = np.linspace(0, array_size-1, grid_size)\n",
        "\n",
        "  # Generate a grid of coordinates\n",
        "  xv, yv = np.meshgrid(x, y)\n",
        "\n",
        "  # Convert the numpy arrays to lists\n",
        "  xv_list = xv.tolist()\n",
        "  yv_list = yv.tolist()\n",
        "\n",
        "  # Combine the x and y coordinates into a list of list of lists\n",
        "  input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv_list, yv_list)]\n",
        "\n",
        "  input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)\n",
        "\n",
        "  return input_points\n",
        "\n",
        "class SAMDataset(Dataset):\n",
        "  \"\"\"\n",
        "  This class is used to create a dataset that serves input images and masks.\n",
        "  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset, processor):\n",
        "    self.dataset = dataset\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.dataset[idx]\n",
        "    image = item[\"image\"]\n",
        "    ground_truth_mask = np.array(item[\"label\"])\n",
        "\n",
        "    # get bounding box prompt\n",
        "    prompt = get_bounding_box(ground_truth_mask)\n",
        "\n",
        "    image = image.convert('RGB')\n",
        "    # prepare image and prompt for the model\n",
        "    inputs = self.processor(image, return_tensors=\"pt\")\n",
        "\n",
        "    inputs[\"input_points\"] = prompt\n",
        "\n",
        "    # remove batch dimension which the processor adds by default\n",
        "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
        "\n",
        "    # add ground truth segmentation\n",
        "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Initialize the processor\n",
        "from transformers import SamProcessor\n",
        "processor = SamProcessor.from_pretrained(\"/content/sam-vit-base\")\n",
        "\n",
        "# Create an instance of the SAMDataset\n",
        "train_dataset = SAMDataset(dataset=dataset, processor=processor)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=False)\n",
        "\n",
        "# Load the model\n",
        "model = SamModel.from_pretrained(\"/content/sam-vit-base\")\n",
        "\n",
        "#  !!only do this if you have a trained model that you want to train further!!\n",
        "model.load_state_dict(torch.load(\"/content/sam_b_cup_green_30.pth\"))\n",
        "\n",
        "# make sure we only compute gradients for mask decoder\n",
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "\n",
        "# Initialize the optimizer and the loss function\n",
        "#optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
        "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-6, weight_decay=0)\n",
        "#Try DiceFocalLoss, FocalLoss, DiceCELoss\n",
        "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
        "\n",
        "#Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"divice: \" + device)\n",
        "model.to(device)\n",
        "\n",
        "epoch_count = 31\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    for batch in tqdm(train_dataloader):\n",
        "      # forward pass\n",
        "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                      input_points=batch[\"input_points\"].to(device),\n",
        "                      multimask_output=False)\n",
        "\n",
        "      # compute loss\n",
        "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "      predicted_masks = F.interpolate(predicted_masks, size=(250, 250), mode='bilinear', align_corners=False)\n",
        "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
        "\n",
        "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # optimize\n",
        "      optimizer.step()\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "    print(f'EPOCH: {epoch_count}')\n",
        "    print(f'Mean loss: {np.mean(epoch_losses)}')\n",
        "\n",
        "    write_header = not os.path.exists(\"sam_green_cup.csv\")\n",
        "\n",
        "    with open(\"sam_green_cup.csv\", mode=\"a\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      if write_header:\n",
        "          writer.writerow([\"time\", \"epoch\", \"loss\"])\n",
        "      writer.writerow([time.perf_counter()-start_time,epoch_count+1, epoch_losses])\n",
        "\n",
        "    f1 = \"/content/sam_b_cup_green_\"+str(epoch_count)+\".pth\"\n",
        "    f0 = \"/content/sam_b_cup_green_\"+str(epoch_count-1)+\".pth\"\n",
        "\n",
        "    torch.save(model.state_dict(), f1)\n",
        "\n",
        "    if os.path.exists(f0):\n",
        "        os.remove(f0)\n",
        "        print(\"file: \"+ \"sam_b_cup_green_\"+str(epoch_count-1)+\".pth\"+ \" updated!\")\n",
        "    else:\n",
        "      print(\"file: \"+ \"sam_b_cup_green_\"+str(epoch_count)+\".pth\"+\" created!\")\n",
        "\n",
        "    epoch_count += 1\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "name": "train_sam_LAB_cup.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
