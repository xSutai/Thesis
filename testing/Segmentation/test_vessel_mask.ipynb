{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import cv2 as cv\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "\n",
    "# Portions of this code were generated with the assistance of ChatGPT (OpenAI, 2025) and subsequently modified by the author.\n",
    "# OpenAI. (2025). ChatGPT (May 2025 version) [Large language model]. https://chat.openai.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"D:/Thesis/SAM/segment-anything/sam_models/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"D:/Thesis/SAM/segment-anything/sam_models/sam-vit-base\")\n",
    "\n",
    "sam = SamModel(config=model_config)\n",
    "#Update the model by loading the weights from saved file.\n",
    "sam.load_state_dict(torch.load(\"D:/Thesis/SAM/segment-anything/notebooks/output_sam_b_brg_vessel_g_L2_v2_50.pth\"))\n",
    "\n",
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(im, mk):\n",
    "\n",
    "    _, binary_mask = cv.threshold(mk, 20, 255, cv.THRESH_BINARY)\n",
    "\n",
    "    masked_image = cv.bitwise_and(im, im, mask=binary_mask)\n",
    "\n",
    "    # Combine them\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "#Apply a trained model on large image\n",
    "\n",
    "testDirImage = \"D:/Downloads/Code/restormer_real_b2_Stijn_very/restormer_fundus_full_real_b2_alligned_ep100/test_latest/images/\"\n",
    "#testDirImage = \"D:/Downloads/Stijn_it2/Stijn_bad_it2/restormer_fundus_full_real_b2_alligned_ep100/test_latest/images/\"\n",
    "#testDirImage = \"D:/Downloads/Stijn/Stijn/Stijn_very_it2/restormer_fundus_full_real_b2_alligned_ep100/test_latest/images/\"\n",
    "#testDirImage = \"D:/Downloads/C/C/\"\n",
    "testDirMask = \"D:/Thesis/datasets/kaggle_drive/test/mask/\"\n",
    "\n",
    "testDirCircle = \"D:/Thesis/datasets/kaggle_drive/test/circle_mask/\"\n",
    "\n",
    "files1 = sorted([f for f in os.listdir(testDirImage) if os.path.isfile(os.path.join(testDirImage, f))])\n",
    "files2 = sorted([f for f in os.listdir(testDirMask) if os.path.isfile(os.path.join(testDirMask, f))])\n",
    "files3 = sorted([f for f in os.listdir(testDirCircle) if os.path.isfile(os.path.join(testDirCircle, f))])\n",
    "\n",
    "test_images = []\n",
    "test_color_images = []\n",
    "test_masks = []\n",
    "\n",
    "j=0\n",
    "for i in range(len(os.listdir(testDirImage))):\n",
    "    if files1[i].endswith('.png'):\n",
    "        img = cv.imread(cv.samples.findFile(testDirImage + files1[i]))\n",
    "        #msk = cv.imread(cv.samples.findFile(testDirCircle + files3[j]), cv.IMREAD_GRAYSCALE)\n",
    "        #img = apply_mask(img, msk)\n",
    "        #cv.imwrite(testDirImage + files1[i], img)\n",
    "        im = cv.cvtColor(img,cv.COLOR_BGR2RGB)\n",
    "        red, green, blue = cv.split(im)\n",
    "        img = np.asarray(green)\n",
    "        test_images += [cv.resize(img,(200,200))] \n",
    "        test_color_images += [cv.resize(im,(256,256))]\n",
    "        j += 1\n",
    "        \n",
    "\n",
    "for i in range(len(os.listdir(testDirMask))):\n",
    "    if files2[i].endswith('.png'):\n",
    "        img = cv.imread(cv.samples.findFile(testDirMask + files2[i]))\n",
    "        img = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "        img = cv.resize(img, (256, 256))  # (width, height)\n",
    "        img = img / 255\n",
    "        test_masks += [img]\n",
    "        test_masks += [img]\n",
    "        test_masks += [img]\n",
    "\n",
    "random = np.random.randint(0,len(test_images))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(len(test_images))\n",
    "print(len(test_masks))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].imshow(np.array(test_color_images[random]), cmap='gray')\n",
    "axes[1].imshow(np.array(test_masks[random]), cmap='gray')\n",
    "plt.show()\n",
    "#patches = patchify(large_test_image, (256, 256), step=256)  #Step=256 for 256 patches means no overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of your array\n",
    "array_size = 200\n",
    "\n",
    "# Define the size of your grid\n",
    "grid_size =15\n",
    "\n",
    "# Generate the grid points\n",
    "x = np.linspace(0, array_size-1, grid_size)\n",
    "y = np.linspace(0, array_size-1, grid_size)\n",
    "\n",
    "# Generate a grid of coordinates\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "# Convert the numpy arrays to lists\n",
    "xv_list = xv.tolist()\n",
    "yv_list = yv.tolist()\n",
    "\n",
    "# Combine the x and y coordinates into a list of list of lists\n",
    "input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv_list, yv_list)]\n",
    "print (input_points)\n",
    "\n",
    "#We need to reshape our nxn grid to the expected shape of the input_points tensor\n",
    "# (batch_size, point_batch_size, num_points_per_image, 2),\n",
    "# where the last dimension of 2 represents the x and y coordinates of each point.\n",
    "#batch_size: The number of images you're processing at once.\n",
    "#point_batch_size: The number of point sets you have for each image.\n",
    "#num_points_per_image: The number of points in each set.\n",
    "input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred_mask, gt_mask, threshold=0.5):\n",
    "    # Binarize the predicted mask if it contains probabilities\n",
    "    pred_bin = (pred_mask > threshold).astype(np.uint8)\n",
    "    gt_bin = (gt_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = np.sum(pred_bin & gt_bin)  # intersection (pred & gt)\n",
    "    union = np.sum(pred_bin | gt_bin)        # union (pred | gt)\n",
    "\n",
    "    # Compute IoU (intersection / union), with a small epsilon to avoid division by zero\n",
    "    iou = intersection / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def accuracy(pred_mask, gt_mask, active_thresh=0.01):\n",
    "    \"\"\"\n",
    "    pred_mask: np.ndarray of shape [1, H, W] or [1, 1, H, W]\n",
    "    gt_mask:   np.ndarray of same shape\n",
    "\n",
    "    Returns: scalar float accuracy over relevant pixels\n",
    "    \"\"\"\n",
    "    if pred_mask.ndim == 4:\n",
    "        pred_mask = pred_mask[0, 0]\n",
    "        gt_mask = gt_mask[0, 0]\n",
    "    elif pred_mask.ndim == 3:\n",
    "        pred_mask = pred_mask[0]\n",
    "        gt_mask = gt_mask[0]\n",
    "\n",
    "    gt_mask = gt_mask[pred_mask > active_thresh]\n",
    "    pred_mask = pred_mask[pred_mask > active_thresh]\n",
    "\n",
    "    l = 1-np.abs(gt_mask - pred_mask)\n",
    "\n",
    "    total = len(l)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "\n",
    "    accuracy = np.sum(l) / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_ap(preds, targets, threshold=0.5, eps=1e-7):\n",
    "    preds_binary = (preds > threshold).astype(np.uint8)\n",
    "    targets = (targets > 0.5).astype(np.uint8)\n",
    "\n",
    "    tp = np.sum(preds_binary * targets)\n",
    "    fp = np.sum(preds_binary * (1 - targets))\n",
    "    fn = np.sum((1 - preds_binary) * targets)\n",
    "\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "\n",
    "    return [f1, precision, recall]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The following named arguments are not valid for `SamImageProcessor.preprocess`.*\")\n",
    "\n",
    "sum_f1 = 0\n",
    "sum_iou = 0\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "# sum_map = [[]for i in range(10)]\n",
    "# map_values = [round(0.5+i*0.05, 2) for i in range(10)]\n",
    "\n",
    "sam.eval()\n",
    "min_f1 = 1\n",
    "max_f1 = 0\n",
    "pic_min = None\n",
    "pic_max = None\n",
    "mk_min = None\n",
    "mk_max = None\n",
    "mk_min_og = None\n",
    "mk_max_og = None\n",
    "im_og = None\n",
    "im_og = None\n",
    "for i in range(len(test_images)):\n",
    "    single_patch = Image.fromarray(test_images[i])\n",
    "\n",
    "    inputs = processor(single_patch.convert(\"RGB\"), input_points=input_points, return_tensors=\"pt\")\n",
    "\n",
    "    # Move the input tensor to the GPU if it's not already there\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = sam(**inputs, multimask_output=False)\n",
    "\n",
    "    # apply sigmoid\n",
    "    single_patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "\n",
    "    single_patch_prob = single_patch_prob.cpu().detach().numpy().squeeze()\n",
    "    f1 = compute_f1_ap(single_patch_prob, test_masks[i])[0]\n",
    "    sum_f1 += f1\n",
    "    #print(\"F1: \", f1)\n",
    "\n",
    "    iou = compute_iou(single_patch_prob, test_masks[i])\n",
    "    quality_score = np.mean([f1, iou])\n",
    "    #print(\"IoU: \", iou)\n",
    "\n",
    "    acc = accuracy(single_patch_prob, test_masks[i])\n",
    "\n",
    "    #print(\"Acc: \", acc)\n",
    "    # sum += acc\n",
    "\n",
    "    if(quality_score < min_f1):\n",
    "        min_f1 = quality_score\n",
    "        pic_min = test_color_images[i]\n",
    "        mk_min = single_patch_prob\n",
    "        mk_min_og = test_masks[i]\n",
    "    if(quality_score > max_f1):\n",
    "        max_f1 = quality_score\n",
    "        pic_max = test_color_images[i]\n",
    "        mk_max = single_patch_prob\n",
    "        mk_max_og = test_masks[i]\n",
    "\n",
    "\n",
    "print(\"total F1: \", sum_f1/len(test_images))\n",
    "#print(\"total Acc: \", sum_acc/len(test_images))\n",
    "# for i in range(len(map_values)):\n",
    "#     ap = np.mean(sum_map[i])\n",
    "#     sum_mAP += ap\n",
    "#     print(\"total AP for threshold \", map_values[i], \": \", ap)\n",
    "# print(\"mAP: \", sum_mAP/len(map_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3964e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images with min and max F1 scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(pic_min)\n",
    "plt.title(f'Min Quality Score: {min_f1:.4f}')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mk_min, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mk_min_og, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(pic_max)\n",
    "plt.title(f'Max Quality Score: {max_f1:.4f}')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mk_max, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mk_max_og, cmap='gray')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
