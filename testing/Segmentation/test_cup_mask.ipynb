{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import cv2 as cv\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "\n",
    "\n",
    "# Portions of this code were generated with the assistance of ChatGPT (OpenAI, 2025) and subsequently modified by the author.\n",
    "# OpenAI. (2025). ChatGPT (May 2025 version) [Large language model]. https://chat.openai.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circular_mask(h, w):\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    center = (w // 2, h // 2)\n",
    "    radius = min(w, h) // 2\n",
    "    cv.circle(mask, center, radius, 1, -1)  # Use 1s instead of 255 for masking arrays\n",
    "    return mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"D:/Thesis/SAM/segment-anything/sam_models/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"D:/Thesis/SAM/segment-anything/sam_models/sam-vit-base\")\n",
    "\n",
    "sam = SamModel(config=model_config)\n",
    "#Update the model by loading the weights from saved file.\n",
    "sam.load_state_dict(torch.load(\"D:/Thesis/SAM/segment-anything/notebooks/output_sam_b_cup_green_cropped_60.pth\"))\n",
    "\n",
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39935a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image):\n",
    "    h, w = image.shape\n",
    "\n",
    "    # Desired crop size\n",
    "    crop_width = 256\n",
    "    crop_height = 256\n",
    "\n",
    "    # Calculate top-left corner of the crop\n",
    "    start_x = (w - crop_width) // 2\n",
    "    start_y = (h - crop_height) // 2\n",
    "\n",
    "    # Ensure the crop stays within image bounds\n",
    "    start_x = max(0, start_x)\n",
    "    start_y = max(0, start_y)\n",
    "    end_x = start_x + crop_width\n",
    "    end_y = start_y + crop_height\n",
    "\n",
    "    # Crop the center\n",
    "    return image[start_y:end_y, start_x:end_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "#Apply a trained model on large image\n",
    "\n",
    "#testDirImage = \"D:/Thesis/datasets/BrG_test_data/cropped_all_im/\"\n",
    "testDirImage = \"D:/Downloads/BrG_test_data_second/very_cr_it2/restormer_fundus_full_real_b2_alligned_ep100/test_latest/images/\"\n",
    "#testDirImage = \"D:/Downloads/BrG_test_data_first/very_c/restormer_fundus_full_real_b2_alligned_ep100/test_latest/images/\"\n",
    "testDirMask = \"D:/Thesis/datasets/BrG_test_data/cropped_mask_cup/\"\n",
    "\n",
    "\n",
    "files1 = sorted([f for f in os.listdir(testDirImage) if os.path.isfile(os.path.join(testDirImage, f))])\n",
    "files2 = sorted([f for f in os.listdir(testDirMask) if os.path.isfile(os.path.join(testDirMask, f))])\n",
    "#files3 = sorted([f for f in os.listdir(testDirCircle) if os.path.isfile(os.path.join(testDirCircle, f))])\n",
    "\n",
    "test_images = []\n",
    "test_color_images = []\n",
    "test_masks = []\n",
    "\n",
    "for i in range(len(os.listdir(testDirImage))):\n",
    "    if files1[i].endswith('real_B.png'):\n",
    "        img = cv.imread(cv.samples.findFile(testDirImage + files1[i]))\n",
    "        test_color_images += [cv.cvtColor(cv.resize(img,(256,256)),cv.COLOR_BGR2RGB)]\n",
    "        im = cv.cvtColor(img,cv.COLOR_BGR2RGB)\n",
    "        R, G, B = cv.split(im)\n",
    "        img = np.asarray(G)\n",
    "        test_images += [cv.resize(img,(256,256))] \n",
    "\n",
    "        \n",
    "for i in range(len(os.listdir(testDirMask))):\n",
    "    if files2[i].endswith('.png'):\n",
    "        img = cv.imread(cv.samples.findFile(testDirMask + files2[i]))\n",
    "        img = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "        #img = crop_image(img)\n",
    "        img = img / 255\n",
    "        test_masks += [cv.resize(img,(256,256))]\n",
    "\n",
    "random = np.random.randint(0,len(test_images))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].imshow(np.array(test_images[random]), cmap='gray')\n",
    "axes[1].imshow(np.array(test_masks[random]), cmap='gray')\n",
    "plt.show()\n",
    "#patches = patchify(large_test_image, (256, 256), step=256)  #Step=256 for 256 patches means no overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputPoints():\n",
    "    array_size = 256\n",
    "    grid_size = 30\n",
    "\n",
    "    # Generate grid points\n",
    "    x = np.linspace(0, array_size - 1, grid_size)\n",
    "    y = np.linspace(0, array_size - 1, grid_size)\n",
    "\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "    # Flatten and zip\n",
    "    input_points = [[int(x), int(y)] for x, y in zip(xv.flatten(), yv.flatten())]\n",
    "\n",
    "    # Wrap in batch format\n",
    "    return [input_points]  # shape: [1, num_points, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred_mask, gt_mask, threshold=0.50):\n",
    "    # Binarize the predicted mask if it contains probabilities\n",
    "    pred_bin = (pred_mask > threshold).astype(np.uint8)\n",
    "    gt_bin = (gt_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = np.sum(pred_bin & gt_bin)  # intersection (pred & gt)\n",
    "    union = np.sum(pred_bin | gt_bin)        # union (pred | gt)\n",
    "\n",
    "    # Compute IoU (intersection / union), with a small epsilon to avoid division by zero\n",
    "    iou = intersection / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def accuracy(pred_mask, gt_mask, active_thresh=0.01):\n",
    "    \"\"\"\n",
    "    pred_mask: np.ndarray of shape [1, H, W] or [1, 1, H, W]\n",
    "    gt_mask:   np.ndarray of same shape\n",
    "\n",
    "    Returns: scalar float accuracy over relevant pixels\n",
    "    \"\"\"\n",
    "    if pred_mask.ndim == 4:\n",
    "        pred_mask = pred_mask[0, 0]\n",
    "        gt_mask = gt_mask[0, 0]\n",
    "    elif pred_mask.ndim == 3:\n",
    "        pred_mask = pred_mask[0]\n",
    "        gt_mask = gt_mask[0]\n",
    "\n",
    "    gt_mask = gt_mask[pred_mask > active_thresh]\n",
    "    pred_mask = pred_mask[pred_mask > active_thresh]\n",
    "\n",
    "    l = 1-np.abs(gt_mask - pred_mask)\n",
    "\n",
    "    total = len(l)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "\n",
    "    accuracy = np.sum(l) / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bc83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_numpy(preds, targets, threshold=0.5, eps=1e-7):\n",
    "    preds_binary = (preds > threshold).astype(np.uint8)\n",
    "    targets = (targets > 0.5).astype(np.uint8)\n",
    "\n",
    "    tp = np.sum(preds_binary * targets)\n",
    "    fp = np.sum(preds_binary * (1 - targets))\n",
    "    fn = np.sum((1 - preds_binary) * targets)\n",
    "\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c59b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The following named arguments are not valid for `SamImageProcessor.preprocess`.*\")\n",
    "\n",
    "sum_f1 = 0\n",
    "sum_acc = 0\n",
    "sum_mAP = 0\n",
    "sum_map = [[]for i in range(10)]\n",
    "map_values = [round(0.5+i*0.05, 2) for i in range(10)]\n",
    "\n",
    "sam.eval()\n",
    "min_f1 = 1\n",
    "max_f1 = 0\n",
    "pic_min = None\n",
    "pic_max = None\n",
    "mk_min = None\n",
    "mk_max = None\n",
    "mk_min_og = None\n",
    "mk_max_og = None\n",
    "im_og = None\n",
    "im_og = None\n",
    "for i in range(len(test_images)):\n",
    "    single_patch = Image.fromarray(test_images[i])\n",
    "\n",
    "    inputs = processor(single_patch.convert(\"RGB\"), input_points=getInputPoints(), return_tensors=\"pt\")\n",
    "\n",
    "    # Move the input tensor to the GPU if it's not already there\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = sam(**inputs, multimask_output=False)\n",
    "\n",
    "    # apply sigmoid\n",
    "    single_patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "\n",
    "    single_patch_prob = single_patch_prob.cpu().detach().numpy().squeeze()\n",
    "    f1 = compute_f1_numpy(single_patch_prob, test_masks[i])\n",
    "    sum_f1 += f1\n",
    "    #print(\"F1: \", f1)\n",
    "\n",
    "    for j in range(len(map_values)):\n",
    "        iou = compute_iou(single_patch_prob, test_masks[i], map_values[j])\n",
    "        sum_map[j] += [iou]\n",
    "    #print(\"IoU: \", iou)\n",
    "\n",
    "    acc = accuracy(single_patch_prob, test_masks[i])\n",
    "    sum_acc += acc\n",
    "    #print(\"Acc: \", acc)\n",
    "    # sum += acc\n",
    "\n",
    "    quality_score = np.mean([f1, iou])\n",
    "\n",
    "    if(quality_score < min_f1):\n",
    "        min_f1 = quality_score\n",
    "        pic_min = test_color_images[i]\n",
    "        mk_min = single_patch_prob\n",
    "        mk_min_og = test_masks[i]\n",
    "    if(quality_score > max_f1):\n",
    "        max_f1 = quality_score\n",
    "        pic_max = test_color_images[i]\n",
    "        mk_max = single_patch_prob\n",
    "        mk_max_og = test_masks[i]\n",
    "\n",
    "\n",
    "print(\"total F1: \", sum_f1/len(test_images))\n",
    "print(\"total Acc: \", sum_acc/len(test_images))\n",
    "for i in range(len(map_values)):\n",
    "    ap = np.mean(sum_map[i])\n",
    "    sum_mAP += ap\n",
    "    print(\"total AP for threshold \", map_values[i], \": \", ap)\n",
    "print(\"mAP: \", sum_mAP/len(map_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images with min and max F1 scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(pic_min)\n",
    "plt.title(f'Min Quality score: {min_f1:.4f}')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mk_min, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mk_min_og, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(pic_max)\n",
    "plt.title(f'Max Quality score: {max_f1:.4f}')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mk_max, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mk_max_og, cmap='gray')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
