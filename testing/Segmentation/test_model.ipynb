{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "# Portions of this code were generated with the assistance of ChatGPT (OpenAI, 2025) and subsequently modified by the author.\n",
    "# OpenAI. (2025). ChatGPT (May 2025 version) [Large language model]. https://chat.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load in the model and trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"D:/Thesis/SAM/segment-anything/sam_models/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"D:/Thesis/SAM/segment-anything/sam_models/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "original_model = SamModel(config=model_config)\n",
    "\n",
    "my_model = SamModel(config=model_config)\n",
    "#Update the model by loading the weights from saved file.\n",
    "#my_model.load_state_dict(torch.load(\"D:/Thesis/SAM/segment-anything/notebooks/output_sam_b_brg_disk_val_80.pth\"))\n",
    "my_model.load_state_dict(torch.load(\"D:/Thesis/SAM/segment-anything/notebooks/output_sam_b_disk_val_24.pth\"))\n",
    "\n",
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "original_model.to(device)\n",
    "my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in test pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "#Apply a trained model on large image\n",
    "\n",
    "testDirImage = \"D:/Thesis/datasets/BrG_test_data/all_bad_im/\"\n",
    "#testDirImage =\"D:/Thesis/datasets/kaggle_drive/test/very_bad_image/\"\n",
    "\n",
    "large_og_images = []\n",
    "large_test_images = []\n",
    "other_images = []\n",
    "\n",
    "# for path in os.listdir(testDirImage2):\n",
    "#     if path.endswith('.png'):\n",
    "#         img = Image.open(testDirImage2 + path)\n",
    "#         red, green, blue = img.split()\n",
    "#         img = np.asarray(green)\n",
    "#         print(img.shape)\n",
    "#         large_test_images += [img]\n",
    "\n",
    "for path in os.listdir(testDirImage):\n",
    "    if path.endswith('.png'):\n",
    "        img = cv.imread(cv.samples.findFile(testDirImage + path))\n",
    "        im = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "        large_og_images += [im]\n",
    "        #red, green, blue = cv.split(im)\n",
    "        other_images += [im]\n",
    "        img = cv.cvtColor(img,cv.COLOR_BGR2HSV)\n",
    "        H, S, V = cv.split(img)\n",
    "        img = np.asarray(V)\n",
    "\n",
    "        #print(img.shape)\n",
    "        large_test_images += [img]\n",
    "\n",
    "random = np.random.randint(0,len(large_test_images))\n",
    "print(random)\n",
    "large_test_image = large_test_images[random]\n",
    "print(np.array(large_test_image).shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(np.array(large_og_images[random]))\n",
    "axes[1].imshow(np.array(other_images[random]), cmap='gray')\n",
    "axes[2].imshow(np.array(large_test_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "plt.show()\n",
    "#patches = patchify(large_test_image, (256, 256), step=256)  #Step=256 for 256 patches means no overlap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an array of input points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of your array\n",
    "array_size = 512\n",
    "\n",
    "# Define the size of your grid\n",
    "grid_size =30\n",
    "\n",
    "# Generate the grid points\n",
    "x = np.linspace(0, array_size-1, grid_size)\n",
    "y = np.linspace(0, array_size-1, grid_size)\n",
    "\n",
    "# Generate a grid of coordinates\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "# Convert the numpy arrays to lists\n",
    "xv_list = xv.tolist()\n",
    "yv_list = yv.tolist()\n",
    "\n",
    "# Combine the x and y coordinates into a list of list of lists\n",
    "input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv_list, yv_list)]\n",
    "print (input_points)\n",
    "\n",
    "#We need to reshape our nxn grid to the expected shape of the input_points tensor\n",
    "# (batch_size, point_batch_size, num_points_per_image, 2),\n",
    "# where the last dimension of 2 represents the x and y coordinates of each point.\n",
    "#batch_size: The number of images you're processing at once.\n",
    "#point_batch_size: The number of point sets you have for each image.\n",
    "#num_points_per_image: The number of points in each set.\n",
    "input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)\n",
    "np.array(input_points).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_model.eval()\n",
    "# Select a random patch for segmentation\n",
    "\n",
    "i = np.random.randint(0,len(large_test_images))\n",
    "\n",
    "# Selectelected patch for segmentation\n",
    "random_array = large_test_images[0]\n",
    "\n",
    "\n",
    "single_patch = Image.fromarray(random_array)\n",
    "single_patch = Image.fromarray(large_test_image)\n",
    "# prepare image for the model\n",
    "\n",
    "#First try without providing any prompt (no bounding box or input_points)\n",
    "#inputs = processor(single_patch.convert(\"RGB\"),  return_tensors=\"pt\")\n",
    "#Now try with bounding boxes. Remember to uncomment.\n",
    "inputs = processor(single_patch.convert(\"RGB\"), input_points=input_points, return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensor to the GPU if it's not already there\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "my_model.eval()\n",
    "\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = my_model(**inputs, multimask_output=False)\n",
    "print(outputs.iou_scores.mean().cpu().numpy())\n",
    "# apply sigmoid\n",
    "single_patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "single_patch_prob = single_patch_prob.cpu().numpy().squeeze()\n",
    "single_patch_prediction = (single_patch_prob > 0.90).astype(np.uint8)\n",
    "\n",
    "print(single_patch_prob.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(single_patch), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(single_patch_prob)  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Probability Map\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[2].imshow(single_patch_prediction, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[2].set_title(\"Prediction\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
